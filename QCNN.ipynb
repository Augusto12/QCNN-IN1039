{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of 8 Qubit QCNN circuit (Quantum Convolutional Neural Networks by Iris Cong et al, 2019) and Tree Tensor Network circuit (Hierarchical Quantum Classifier by Ed Grant et al, 2018) with the comparisons among various ansatze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCNN Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are various unitary ansatze to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitraies for Convolutional Layers \n",
    "def U_TTN(params, wires): # 2 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_5(params, wires): # 10 params\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "    qml.RZ(params[2], wires = wires[0])\n",
    "    qml.RZ(params[3], wires = wires[1])\n",
    "    qml.CRZ(params[4], wires = [wires[1], wires[0]])\n",
    "    qml.CRZ(params[5], wires = [wires[0], wires[1]])\n",
    "    qml.RX(params[6], wires = wires[0])\n",
    "    qml.RX(params[7], wires = wires[1])\n",
    "    qml.RZ(params[8], wires = wires[0])\n",
    "    qml.RZ(params[9], wires = wires[1])\n",
    "    \n",
    "def U_6(params, wires): # 10 params\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "    qml.RZ(params[2], wires = wires[0])\n",
    "    qml.RZ(params[3], wires = wires[1])\n",
    "    qml.CRX(params[4], wires = [wires[1], wires[0]])\n",
    "    qml.CRX(params[5], wires = [wires[0], wires[1]])\n",
    "    qml.RX(params[6], wires = wires[0])\n",
    "    qml.RX(params[7], wires = wires[1])\n",
    "    qml.RZ(params[8], wires = wires[0])\n",
    "    qml.RZ(params[9], wires = wires[1])\n",
    "\n",
    "def U_9(params, wires): # 2 params\n",
    "    qml.Hadamard(wires = wires[0])\n",
    "    qml.Hadamard(wires = wires[1])\n",
    "    qml.CZ(wires = [wires[0],wires[1]])\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "\n",
    "def U_13(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CRZ(params[2], wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[3], wires = wires[0])\n",
    "    qml.RY(params[4], wires = wires[1])\n",
    "    qml.CRZ(params[5], wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_14(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CRX(params[2], wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[3], wires = wires[0])\n",
    "    qml.RY(params[4], wires = wires[1])\n",
    "    qml.CRX(params[5], wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_15(params, wires): # 4 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[2], wires = wires[0])\n",
    "    qml.RY(params[3], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_SO4(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "    qml.RY(params[2], wires = wires[0])\n",
    "    qml.RY(params[3], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "    qml.RY(params[4], wires = wires[0])\n",
    "    qml.RY(params[5], wires = wires[1])\n",
    "\n",
    "# Unitraies for Pooling and Fully Connected Layers\n",
    "def V_0(theta, wires):\n",
    "    qml.CRZ(theta, wires = [wires[0], wires[1]])\n",
    "\n",
    "def V_1(theta, wires):\n",
    "    qml.CRX(theta, wires = [wires[0], wires[1]])\n",
    "\n",
    "def F(theta, wires):\n",
    "    qml.CRZ(theta, wires = [wires[0], wires[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is general circuit structures used in Quantum Convolutional Neural Network (QCNN) circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer1\n",
    "def conv_layer1(U, params):\n",
    "    U(params, wires = [0,7])\n",
    "    for i in range (0,8,2):\n",
    "        U(params, wires = [i, i+1])\n",
    "    for i in range (1,7,2):\n",
    "        U(params, wires = [i, i+1])\n",
    "    \n",
    "def conv_layer2(U, params):\n",
    "    U(params, wires = [0,2])\n",
    "    U(params, wires = [4,6])\n",
    "    U(params, wires = [2,4])\n",
    "    U(params, wires = [0,6])\n",
    "    \n",
    "def pooling_layer1(V_0, V_1, params):\n",
    "    for i in range(0,8,2):\n",
    "        V_0(params[0], wires = [i+1, i])\n",
    "    for i in range(0,8,2):\n",
    "        qml.PauliX(wires = i+1)\n",
    "    for i in range(0,8,2):\n",
    "        V_1(params[1], wires = [i+1, i])\n",
    "        \n",
    "\n",
    "def pooling_layer2(V_0, V_1, params): # 2params\n",
    "    V_0(params[0], wires = [2,0])\n",
    "    V_0(params[0], wires = [6,4])\n",
    "    \n",
    "    qml.PauliX(wires = 2)\n",
    "    qml.PauliX(wires = 6)\n",
    "    \n",
    "    V_1(params[1], wires = [2,0])\n",
    "    V_1(params[1], wires = [6,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define various possible embedding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.templates.embeddings import AmplitudeEmbedding, AngleEmbedding\n",
    "\n",
    "def data_embedding(X, embedding_type = 'Amplitude'):\n",
    "    if embedding_type == 'Amplitude':\n",
    "        AmplitudeEmbedding(X, wires = range(8), normalize = True)\n",
    "    elif embedding_type == 'Angle':\n",
    "        AngleEmbedding(X, wires = range(8), rotation = 'Y')\n",
    "    elif embedding_type == 'Hybrid':\n",
    "        X1 = X[:2**4]\n",
    "        X2 = X[2**4:2**5]\n",
    "        AmplitudeEmbedding(X1, wires = range(4), normalize = True)\n",
    "        AmplitudeEmbedding(X2, wires = range(4,8), normalize = True)\n",
    "    elif embedding_type == 'Hybrid16':\n",
    "        X1 = X[:4]\n",
    "        X2 = X[4:8]\n",
    "        X3 = X[8:12]\n",
    "        X4 = X[12:16]\n",
    "        AmplitudeEmbedding(X1, wires = range(2), normalize = True)\n",
    "        AmplitudeEmbedding(X2, wires = range(2,4), normalize = True)\n",
    "        AmplitudeEmbedding(X3, wires = range(4,6), normalize = True)\n",
    "        AmplitudeEmbedding(X4, wires = range(6,8), normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define QCNN circuit with given Unitary Ansatz and embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires = 8)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def QCNN(X, params, U, U_params, embedding_type = 'Amplitude'):\n",
    "\n",
    "    param1 = params[0:U_params]\n",
    "    param2 = params[U_params:U_params + 2]\n",
    "    param3 = params[U_params + 2: 2*U_params + 2]\n",
    "    param4 = params[2*U_params + 2: 2*U_params + 4]\n",
    "    param5 = params[2*U_params + 4]\n",
    "    \n",
    "    # Data Embedding\n",
    "    data_embedding(X, embedding_type = embedding_type)\n",
    "    \n",
    "    #Quantum Convolutional Neural Network\n",
    "    if U == 'U_TTN':\n",
    "        conv_layer1(U_TTN, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_TTN, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_5':\n",
    "        conv_layer1(U_5, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_5, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_6':\n",
    "        conv_layer1(U_6, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_6, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_9':\n",
    "        conv_layer1(U_9, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_9, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_13':\n",
    "        conv_layer1(U_13, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_13, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_14':\n",
    "        conv_layer1(U_14, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_14, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_15':\n",
    "        conv_layer1(U_15, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_15, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "    \n",
    "    elif U == 'U_SO4':\n",
    "        conv_layer1(U_SO4, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_SO4, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Hierarchical Quantum Classifier structure with different Ansatze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_TTN = qml.device('default.qubit', wires = 8)\n",
    "\n",
    "@qml.qnode(dev_TTN)\n",
    "def Hierarchical_classifier(X, params, U, U_params, embedding_type = 'Amplitude'):\n",
    "    \n",
    "    param1 = params[0 * U_params:1 * U_params]\n",
    "    param2 = params[1 * U_params:2 * U_params]\n",
    "    param3 = params[2 * U_params:3 * U_params]\n",
    "    param4 = params[3 * U_params:4 * U_params]\n",
    "    param5 = params[4 * U_params:5 * U_params]\n",
    "    param6 = params[5 * U_params:6 * U_params]\n",
    "    param7 = params[6 * U_params:7 * U_params]\n",
    "    \n",
    "    data_embedding(X, embedding_type = embedding_type) \n",
    "    if U == 'U_TTN':\n",
    "        # layer 1\n",
    "        U_TTN(param1, wires = [0,1])\n",
    "        U_TTN(param2, wires = [2,3])\n",
    "        U_TTN(param3, wires = [4,5])\n",
    "        U_TTN(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_TTN(param5, wires = [1,3])\n",
    "        U_TTN(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_TTN(param7, wires = [3,7])\n",
    "    elif U == 'U_5':\n",
    "        # layer 1\n",
    "        U_5(param1, wires = [0,1])\n",
    "        U_5(param2, wires = [2,3])\n",
    "        U_5(param3, wires = [4,5])\n",
    "        U_5(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_5(param5, wires = [1,3])\n",
    "        U_5(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_5(param7, wires = [3,7])\n",
    "    elif U == 'U_6':\n",
    "        # layer 1\n",
    "        U_6(param1, wires = [0,1])\n",
    "        U_6(param2, wires = [2,3])\n",
    "        U_6(param3, wires = [4,5])\n",
    "        U_6(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_6(param5, wires = [1,3])\n",
    "        U_6(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_6(param7, wires = [3,7])\n",
    "    elif U == 'U_13':\n",
    "        # layer 1\n",
    "        U_13(param1, wires = [0,1])\n",
    "        U_13(param2, wires = [2,3])\n",
    "        U_13(param3, wires = [4,5])\n",
    "        U_13(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_13(param5, wires = [1,3])\n",
    "        U_13(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_13(param7, wires = [3,7])\n",
    "    elif U == 'U_14':\n",
    "        # layer 1\n",
    "        U_14(param1, wires = [0,1])\n",
    "        U_14(param2, wires = [2,3])\n",
    "        U_14(param3, wires = [4,5])\n",
    "        U_14(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_14(param5, wires = [1,3])\n",
    "        U_14(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_14(param7, wires = [3,7])\n",
    "    elif U == 'U_15':\n",
    "        # layer 1\n",
    "        U_15(param1, wires = [0,1])\n",
    "        U_15(param2, wires = [2,3])\n",
    "        U_15(param3, wires = [4,5])\n",
    "        U_15(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_15(param5, wires = [1,3])\n",
    "        U_15(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_15(param7, wires = [3,7])\n",
    "    elif U == 'U_SO4':\n",
    "        # layer 1\n",
    "        U_SO4(param1, wires = [0,1])\n",
    "        U_SO4(param2, wires = [2,3])\n",
    "        U_SO4(param3, wires = [4,5])\n",
    "        U_SO4(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_SO4(param5, wires = [1,3])\n",
    "        U_SO4(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_SO4(param7, wires = [3,7])\n",
    "    \n",
    "\n",
    "    return qml.expval(qml.PauliZ(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Quantum Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(params, X, Y, U, U_params, embedding_type, circuit):\n",
    "    if circuit == 'QCNN':\n",
    "        predictions = [QCNN(x, params, U, U_params, embedding_type) for x in X]\n",
    "    elif circuit == 'Hierarchical':\n",
    "        predictions = [Hierarchical_classifier(x, params, U, U_params, embedding_type) for x in X]\n",
    "    \n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "def accuracy_test_binary(predictions, labels):\n",
    "    acc = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 1:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)\n",
    "\n",
    "def accuracy_test_one_class(predictions, labels):\n",
    "    acc = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 0.5:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_training(X_train, Y_train, U, U_params, embedding_type, circuit):\n",
    "    if circuit == 'QCNN':\n",
    "        total_params = U_params * 2 + 2 * 2 + 1\n",
    "    elif circuit == 'Hierarchical':\n",
    "        total_params = U_params * 7\n",
    "        \n",
    "    params = np.random.randn(total_params)\n",
    "    steps = 150\n",
    "    learning_rate = 0.1\n",
    "    batch_size =25\n",
    "    opt = qml.NesterovMomentumOptimizer(learning_rate)\n",
    "    \n",
    "    for it in range(steps):\n",
    "        batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "        X_batch = [X_train[i] for i in batch_index]\n",
    "        Y_batch = [Y_train[i] for i in batch_index]\n",
    "        params, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U, U_params, embedding_type, circuit), params)\n",
    "        if it % 10 == 0:\n",
    "            print(\"iteration: \", it, \" cost: \", cost_new)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data loading and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA and Autoencoder to reduce it into 8 features. We test both one-class classification (labeling 0 and 1) and binary classification (labeling -1 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "def data_load_and_process(dataset, classes = [0,1], feature_reduction = 'resize256', binary = True):\n",
    "    \n",
    "    if dataset == 'fashion_mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    elif dataset == 'mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0 #normalize the data\n",
    "    \n",
    "    x_train_filter_01 = np.where((y_train == classes[0]) | (y_train == classes[1]))\n",
    "    x_test_filter_01 = np.where((y_test == classes[0]) | (y_test == classes[1]))\n",
    "    \n",
    "    x_train_01, x_test_01 = x_train[x_train_filter_01], x_test[x_test_filter_01]\n",
    "    y_train_01, y_test_01 = y_train[x_train_filter_01], y_test[x_test_filter_01]\n",
    "    \n",
    "    if binary == False:\n",
    "        y_train_01 = [1 if y ==classes[0] else 0 for y in y_train_01]\n",
    "        y_test_01 = [1 if y ==classes[0] else 0 for y in y_test_01]\n",
    "    elif binary == True:\n",
    "        y_train_01 = [1 if y ==classes[0] else -1 for y in y_train_01]\n",
    "        y_test_01 = [1 if y ==classes[0] else -1 for y in y_test_01]\n",
    "        \n",
    "    \n",
    "    if feature_reduction == 'resize256':   \n",
    "        x_train_01 = tf.image.resize(x_train_01[:], (256, 1)).numpy()\n",
    "        x_test_01 = tf.image.resize(x_test_01[:], (256, 1)).numpy()\n",
    "        x_train_01, x_test_01 = tf.squeeze(x_train_01), tf.squeeze(x_test_01) \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "    \n",
    "    ##############################\n",
    "    #This part of Code is a test section to test hybrid embedding method\n",
    "    elif feature_reduction == 'pca16':\n",
    "        x_train_01 = tf.image.resize(x_train_01[:], (784, 1)).numpy()\n",
    "        x_test_01 = tf.image.resize(x_test_01[:], (784, 1)).numpy()\n",
    "        x_train_01, x_test_01 = tf.squeeze(x_train_01), tf.squeeze(x_test_01)\n",
    "        \n",
    "        pca = PCA(16)\n",
    "        x_train_01 = pca.fit_transform(x_train_01)\n",
    "        x_test_01 = pca.transform(x_test_01)\n",
    "        \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "    ##############################\n",
    "    \n",
    "    elif feature_reduction == 'pca8' or feature_reduction == 'pca32':\n",
    "        x_train_01 = tf.image.resize(x_train_01[:], (784, 1)).numpy()\n",
    "        x_test_01 = tf.image.resize(x_test_01[:], (784, 1)).numpy()\n",
    "        x_train_01, x_test_01 = tf.squeeze(x_train_01), tf.squeeze(x_test_01)\n",
    "        \n",
    "        if feature_reduction == 'pca8':\n",
    "            pca = PCA(8)\n",
    "        elif feature_reduction == 'pca32':\n",
    "            pca = PCA(32)\n",
    "            \n",
    "        x_train_01 = pca.fit_transform(x_train_01)\n",
    "        x_test_01 = pca.transform(x_test_01)\n",
    "        \n",
    "        #Rescale for angle embedding\n",
    "        if feature_reduction == 'pca8':\n",
    "            x_train_01, x_test_01 = (x_train_01 + 10) * (np.pi / 20), (x_test_01 + 10) * (np.pi / 20)\n",
    "        \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "\n",
    "    elif feature_reduction == 'autoencoder8' or feature_reduction == 'autoencoder32':\n",
    "        if feature_reduction == 'autoencoder8':\n",
    "            latent_dim = 8 \n",
    "        elif feature_reduction == 'autoencoder32':\n",
    "            latent_dim = 32\n",
    "            \n",
    "        class Autoencoder(Model):\n",
    "            def __init__(self, latent_dim):\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.latent_dim = latent_dim   \n",
    "                self.encoder = tf.keras.Sequential([\n",
    "                layers.Flatten(),\n",
    "                  layers.Dense(latent_dim, activation='relu'),\n",
    "                ])\n",
    "                self.decoder = tf.keras.Sequential([\n",
    "                layers.Dense(784, activation='sigmoid'),\n",
    "                layers.Reshape((28, 28))\n",
    "                ])\n",
    "            def call(self, x):\n",
    "                encoded = self.encoder(x)\n",
    "                decoded = self.decoder(encoded)\n",
    "                return decoded\n",
    "        \n",
    "        autoencoder = Autoencoder(latent_dim)\n",
    "        \n",
    "        autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "        autoencoder.fit(x_train_01, x_train_01,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_01, x_test_01))\n",
    "        \n",
    "        x_train_01, x_test_01 = autoencoder.encoder(x_train_01).numpy(), autoencoder.encoder(x_test_01).numpy()\n",
    "        #Rescale for Angle Embedding\n",
    "        if feature_reduction == 'autoencoder8':\n",
    "            x_train_01, x_test_01 = x_train_01 * (np.pi / 50), x_test_01 * (np.pi / 50)\n",
    "        \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking function trains and performs accuracy tests for all the given unitary ansatze, feature reduction methods, and circuit structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Benchmarking(dataset, Unitaries, U_num_params, Encodings, circuit, binary = True):\n",
    "    I = len(Unitaries)\n",
    "    J = len(Encodings)\n",
    "    All_predictions = []\n",
    "    \n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            U = Unitaries[i]\n",
    "            U_params = U_num_params[i]\n",
    "            Encoding = Encodings[j]\n",
    "            if Encoding == 'resize256':\n",
    "                Embedding = 'Amplitude'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(dataset, classes = classes, feature_reduction = 'resize256', binary = binary)\n",
    "            elif Encoding == 'pca8':\n",
    "                Embedding = 'Angle'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(dataset, classes = classes, feature_reduction = 'pca8', binary = binary)\n",
    "            elif Encoding == 'autoencoder8':\n",
    "                Embedding = 'Angle'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(dataset, classes = classes, feature_reduction = 'autoencoder8', binary = binary)\n",
    "            elif Encoding == 'pca32':\n",
    "                Embedding = 'Hybird'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(dataset, classes = classes, feature_reduction = 'pca32', binary = binary)\n",
    "            elif Encoding == 'autoencoder32':\n",
    "                Embedding = 'Hybrid'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(dataset, classes = classes, feature_reduction = 'autoencoder32', binary = binary)\n",
    "            print(\"\\n\")\n",
    "            print(\"Loss History for \" + circuit + \" circuits, \"+ U + \" \" + Encoding)\n",
    "            trained_params = circuit_training(X_train, Y_train, U, U_params, Embedding, circuit)\n",
    "            \n",
    "            if circuit == 'QCNN':\n",
    "                predictions = [QCNN(x, trained_params, U, U_params, Embedding) for x in X_test]\n",
    "            elif circuit == 'Hierarchical':\n",
    "                predictions = [Hierarchical_classifier(x, trained_params, U, U_params, Embedding) for x in X_test]\n",
    "                \n",
    "                \n",
    "            if binary == True:\n",
    "                accuracy = accuracy_test_binary(predictions, Y_test)\n",
    "            elif binary == False:\n",
    "                accuracy = accuracy_test_one_class(predictions, Y_test)\n",
    "                \n",
    "            print(\"Accuracy for \" + U + \" \" + Encoding + \" :\" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the result of the benchmarking code showing the accuracies of the variations of quantum classifiers. There are 5 variables that user can control fro benchmarking. \n",
    "\n",
    "\"dataset\" controls the classical dataset to classify.\n",
    "\n",
    "\"classes\" chooses two classes to classify from the dataset.\n",
    "\n",
    "\"Unitaries\" controls the 2-qubits unitary operator that used in a quantum circuit.\n",
    "\n",
    "\"Encodings\" controls the how to prepare the classical data and initial quantum state preparation.\n",
    "\n",
    "\"circuit\" controls the circuit structure to use.\n",
    "\n",
    "If binary is true, two classes are labelled as 1 and -1 (Corresponding to 0 and 1 qubit).\n",
    "\n",
    "If binary is false, two classes are labelled as 1 and 0 (Corresponding to 0 and ambiguous equator qubit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Binary Classification with 1, -1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_9', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "U_num_params = [2, 10, 10, 4, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'QCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = True)\n",
    "#Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results for Hierarchical Classifier circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Binary Classification with 1, -1 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "U_num_params = [2, 10, 10, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'Hierarchical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = True)\n",
    "#Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results for Hybrid Embedding Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#This is a test section erase. It is not an official code\n",
    "dataset = 'fashion_mnist'\n",
    "Unitaries = ['U_5', 'U_15', 'U_SO4']\n",
    "U_num_params = [10, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'QCNN'\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_5 resize256\n",
      "iteration:  0  cost:  1.0815941811883232\n",
      "iteration:  10  cost:  1.0021581286716588\n",
      "iteration:  20  cost:  0.9947537770580227\n",
      "iteration:  30  cost:  0.9146177110177968\n",
      "iteration:  40  cost:  0.8335432368113903\n",
      "iteration:  50  cost:  0.7149888996878089\n",
      "iteration:  60  cost:  0.5966592888329987\n",
      "iteration:  70  cost:  0.44084768768627114\n",
      "iteration:  80  cost:  0.48206103933711547\n",
      "iteration:  90  cost:  0.5290650861702665\n",
      "iteration:  100  cost:  0.429556861337692\n",
      "iteration:  110  cost:  0.5028447962778231\n",
      "iteration:  120  cost:  0.6248220407890535\n",
      "iteration:  130  cost:  0.39034693434457624\n",
      "iteration:  140  cost:  0.5323411286999996\n",
      "Accuracy for U_5 resize256 :0.918\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_5 pca8\n",
      "iteration:  0  cost:  1.1944612702812412\n",
      "iteration:  10  cost:  0.9622891985765785\n",
      "iteration:  20  cost:  0.5145022591594335\n",
      "iteration:  30  cost:  0.480356817869803\n",
      "iteration:  40  cost:  0.36127901103257654\n",
      "iteration:  50  cost:  0.4731131866822397\n",
      "iteration:  60  cost:  0.44137647786674283\n",
      "iteration:  70  cost:  0.35883424443850337\n",
      "iteration:  80  cost:  0.5842055987490534\n",
      "iteration:  90  cost:  0.45340659116255516\n",
      "iteration:  100  cost:  0.377082457369534\n",
      "iteration:  110  cost:  0.43541447341711553\n",
      "iteration:  120  cost:  0.5316511138698372\n",
      "iteration:  130  cost:  0.39588395108587315\n",
      "iteration:  140  cost:  0.4387771326046165\n",
      "Accuracy for U_5 pca8 :0.8995\n",
      "Train on 12000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 1s 63us/sample - loss: 0.0515 - val_loss: 0.0331\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 43us/sample - loss: 0.0291 - val_loss: 0.0266\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 50us/sample - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 44us/sample - loss: 0.0207 - val_loss: 0.0199\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 0.0180 - val_loss: 0.0178\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 0s 41us/sample - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 0.0160 - val_loss: 0.0161\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_5 autoencoder8\n",
      "iteration:  0  cost:  1.644848242100679\n",
      "iteration:  10  cost:  0.5814320718484078\n",
      "iteration:  20  cost:  0.5205318587411262\n",
      "iteration:  30  cost:  0.4202587545830366\n",
      "iteration:  40  cost:  0.3175180312432464\n",
      "iteration:  50  cost:  0.5656258778280372\n",
      "iteration:  60  cost:  0.4173582057144904\n",
      "iteration:  70  cost:  0.39567769046018925\n",
      "iteration:  80  cost:  0.4510261561442755\n",
      "iteration:  90  cost:  0.30710622977089314\n",
      "iteration:  100  cost:  0.5350575753160197\n",
      "iteration:  110  cost:  0.32676702434082683\n",
      "iteration:  120  cost:  0.4224838528950051\n",
      "iteration:  130  cost:  0.38705597643819606\n",
      "iteration:  140  cost:  0.39241572868996116\n",
      "Accuracy for U_5 autoencoder8 :0.9375\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_15 resize256\n",
      "iteration:  0  cost:  1.0101648735428994\n",
      "iteration:  10  cost:  0.8832324607454857\n",
      "iteration:  20  cost:  0.6760555925137467\n",
      "iteration:  30  cost:  0.5386847107735229\n",
      "iteration:  40  cost:  0.5878189505092607\n",
      "iteration:  50  cost:  0.47900941490067966\n",
      "iteration:  60  cost:  0.47003413321535303\n",
      "iteration:  70  cost:  0.3808140811183157\n",
      "iteration:  80  cost:  0.35287337952544134\n",
      "iteration:  90  cost:  0.23252437411599472\n",
      "iteration:  100  cost:  0.22213866238666471\n",
      "iteration:  110  cost:  0.3617067068086257\n",
      "iteration:  120  cost:  0.5215468042975068\n",
      "iteration:  130  cost:  0.5867335474837766\n",
      "iteration:  140  cost:  0.4028101352086329\n",
      "Accuracy for U_15 resize256 :0.9075\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_15 pca8\n",
      "iteration:  0  cost:  1.1225346661186453\n",
      "iteration:  10  cost:  0.6999206365076747\n",
      "iteration:  20  cost:  0.18787139231096636\n",
      "iteration:  30  cost:  0.34460524050234725\n",
      "iteration:  40  cost:  0.5263443146461593\n",
      "iteration:  50  cost:  0.5129292330853042\n",
      "iteration:  60  cost:  0.37902966626556567\n",
      "iteration:  70  cost:  0.33031069827574017\n",
      "iteration:  80  cost:  0.3965116704955449\n",
      "iteration:  90  cost:  0.3936807773778737\n",
      "iteration:  100  cost:  0.43573393787014053\n",
      "iteration:  110  cost:  0.3153567344962005\n",
      "iteration:  120  cost:  0.21084119135111043\n",
      "iteration:  130  cost:  0.3405030345816122\n",
      "iteration:  140  cost:  0.4552463084758462\n",
      "Accuracy for U_15 pca8 :0.914\n",
      "Train on 12000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 1s 85us/sample - loss: 0.0495 - val_loss: 0.0330\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 60us/sample - loss: 0.0298 - val_loss: 0.0271\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 59us/sample - loss: 0.0234 - val_loss: 0.0210\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 60us/sample - loss: 0.0195 - val_loss: 0.0188\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 57us/sample - loss: 0.0179 - val_loss: 0.0176\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 63us/sample - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 66us/sample - loss: 0.0162 - val_loss: 0.0161\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 60us/sample - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 62us/sample - loss: 0.0149 - val_loss: 0.0149\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_15 autoencoder8\n",
      "iteration:  0  cost:  1.007671215454195\n",
      "iteration:  10  cost:  0.6991269094147315\n",
      "iteration:  20  cost:  0.6002704247720956\n",
      "iteration:  30  cost:  0.6737224340890333\n",
      "iteration:  40  cost:  0.5698845401278901\n",
      "iteration:  50  cost:  0.44248326857980963\n",
      "iteration:  60  cost:  0.6186502949141607\n",
      "iteration:  70  cost:  0.459464491098307\n",
      "iteration:  80  cost:  0.40067327539471603\n",
      "iteration:  90  cost:  0.4877660622337572\n",
      "iteration:  100  cost:  0.3667103751434264\n",
      "iteration:  110  cost:  0.45111992163971903\n",
      "iteration:  120  cost:  0.42145988665680795\n",
      "iteration:  130  cost:  0.47804617477869327\n",
      "iteration:  140  cost:  0.4923993922851168\n",
      "Accuracy for U_15 autoencoder8 :0.9465\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_SO4 resize256\n",
      "iteration:  0  cost:  1.0216778020223105\n",
      "iteration:  10  cost:  1.0288497235248582\n",
      "iteration:  20  cost:  0.9501832080316145\n",
      "iteration:  30  cost:  0.728470679662491\n",
      "iteration:  40  cost:  0.7271771380475026\n",
      "iteration:  50  cost:  0.6007417232120514\n",
      "iteration:  60  cost:  0.6056936875434428\n",
      "iteration:  70  cost:  0.33857374478060775\n",
      "iteration:  80  cost:  0.39748654986554477\n",
      "iteration:  90  cost:  0.6217640058005545\n",
      "iteration:  100  cost:  0.5504882386940512\n",
      "iteration:  110  cost:  0.7545409811701624\n",
      "iteration:  120  cost:  0.547639670393753\n",
      "iteration:  130  cost:  0.3746823670743879\n",
      "iteration:  140  cost:  0.578252222404286\n",
      "Accuracy for U_SO4 resize256 :0.916\n",
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_SO4 pca8\n",
      "iteration:  0  cost:  1.0384486596759621\n",
      "iteration:  10  cost:  0.7281863719240674\n",
      "iteration:  20  cost:  0.5854586997920622\n",
      "iteration:  30  cost:  0.5319469041871048\n",
      "iteration:  40  cost:  0.5668743626393424\n",
      "iteration:  50  cost:  0.5221735847849189\n",
      "iteration:  60  cost:  0.5383972720990208\n",
      "iteration:  70  cost:  0.4787851074913741\n",
      "iteration:  80  cost:  0.6662457557823308\n",
      "iteration:  90  cost:  0.4269688504249043\n",
      "iteration:  100  cost:  0.48981719396752316\n",
      "iteration:  110  cost:  0.40848349605389894\n",
      "iteration:  120  cost:  0.5661722229511328\n",
      "iteration:  130  cost:  0.5746982998285246\n",
      "iteration:  140  cost:  0.49709904181849424\n",
      "Accuracy for U_SO4 pca8 :0.862\n",
      "Train on 12000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 1s 66us/sample - loss: 0.0509 - val_loss: 0.0316\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 47us/sample - loss: 0.0291 - val_loss: 0.0272\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 46us/sample - loss: 0.0249 - val_loss: 0.0230\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 46us/sample - loss: 0.0212 - val_loss: 0.0203\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 47us/sample - loss: 0.0191 - val_loss: 0.0186\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 47us/sample - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 48us/sample - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 47us/sample - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 47us/sample - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 48us/sample - loss: 0.0157 - val_loss: 0.0156\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for QCNN circuits, U_SO4 autoencoder8\n",
      "iteration:  0  cost:  1.0126474542835704\n",
      "iteration:  10  cost:  0.7263575010950395\n",
      "iteration:  20  cost:  0.6780532638174662\n",
      "iteration:  30  cost:  0.6176079098329583\n",
      "iteration:  40  cost:  0.606410607003926\n",
      "iteration:  50  cost:  0.5993053137324965\n",
      "iteration:  60  cost:  0.7205674889400648\n",
      "iteration:  70  cost:  0.5731016636872155\n",
      "iteration:  80  cost:  0.6942308911072379\n",
      "iteration:  90  cost:  0.611872884328998\n",
      "iteration:  100  cost:  0.6667923363677167\n",
      "iteration:  110  cost:  0.681263848169506\n",
      "iteration:  120  cost:  0.5763333427031858\n",
      "iteration:  130  cost:  0.7102592870204437\n",
      "iteration:  140  cost:  0.6484608563926969\n",
      "Accuracy for U_SO4 autoencoder8 :0.8815\n"
     ]
    }
   ],
   "source": [
    "Benchmarking(dataset, Unitaries, U_num_params, Encodings, circuit, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
