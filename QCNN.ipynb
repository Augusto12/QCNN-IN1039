{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of 8 Qubit QCNN circuit (Quantum Convolutional Neural Networks by Iris Cong et al, 2019) and Tree Tensor Network circuit (Hierarchical Quantum Classifier by Ed Grant et al, 2018) with the comparisons among various ansatze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCNN Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are various unitary ansatze to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitraies for Convolutional Layers \n",
    "def U_TTN(params, wires): # 2 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_5(params, wires): # 10 params\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "    qml.RZ(params[2], wires = wires[0])\n",
    "    qml.RZ(params[3], wires = wires[1])\n",
    "    qml.CRZ(params[4], wires = [wires[1], wires[0]])\n",
    "    qml.CRZ(params[5], wires = [wires[0], wires[1]])\n",
    "    qml.RX(params[6], wires = wires[0])\n",
    "    qml.RX(params[7], wires = wires[1])\n",
    "    qml.RZ(params[8], wires = wires[0])\n",
    "    qml.RZ(params[9], wires = wires[1])\n",
    "    \n",
    "def U_6(params, wires): # 10 params\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "    qml.RZ(params[2], wires = wires[0])\n",
    "    qml.RZ(params[3], wires = wires[1])\n",
    "    qml.CRX(params[4], wires = [wires[1], wires[0]])\n",
    "    qml.CRX(params[5], wires = [wires[0], wires[1]])\n",
    "    qml.RX(params[6], wires = wires[0])\n",
    "    qml.RX(params[7], wires = wires[1])\n",
    "    qml.RZ(params[8], wires = wires[0])\n",
    "    qml.RZ(params[9], wires = wires[1])\n",
    "\n",
    "def U_9(params, wires): # 2 params\n",
    "    qml.Hadamard(wires = wires[0])\n",
    "    qml.Hadamard(wires = wires[1])\n",
    "    qml.CZ(wires = [wires[0],wires[1]])\n",
    "    qml.RX(params[0], wires = wires[0])\n",
    "    qml.RX(params[1], wires = wires[1])\n",
    "\n",
    "def U_13(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CRZ(params[2], wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[3], wires = wires[0])\n",
    "    qml.RY(params[4], wires = wires[1])\n",
    "    qml.CRZ(params[5], wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_14(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CRX(params[2], wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[3], wires = wires[0])\n",
    "    qml.RY(params[4], wires = wires[1])\n",
    "    qml.CRX(params[5], wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_15(params, wires): # 4 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[1], wires[0]])\n",
    "    qml.RY(params[2], wires = wires[0])\n",
    "    qml.RY(params[3], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "\n",
    "def U_SO4(params, wires): # 6 params\n",
    "    qml.RY(params[0], wires = wires[0])\n",
    "    qml.RY(params[1], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "    qml.RY(params[2], wires = wires[0])\n",
    "    qml.RY(params[3], wires = wires[1])\n",
    "    qml.CNOT(wires = [wires[0], wires[1]])\n",
    "    qml.RY(params[4], wires = wires[0])\n",
    "    qml.RY(params[5], wires = wires[1])\n",
    "\n",
    "# Unitraies for Pooling and Fully Connected Layers\n",
    "def V_0(theta, wires):\n",
    "    qml.CRZ(theta, wires = [wires[0], wires[1]])\n",
    "\n",
    "def V_1(theta, wires):\n",
    "    qml.CRX(theta, wires = [wires[0], wires[1]])\n",
    "\n",
    "def F(theta, wires):\n",
    "    qml.CRZ(theta, wires = [wires[0], wires[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is general circuit structures used in Quantum Convolutional Neural Network (QCNN) circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer1\n",
    "def conv_layer1(U, params):\n",
    "    U(params, wires = [0,7])\n",
    "    for i in range (0,8,2):\n",
    "        U(params, wires = [i, i+1])\n",
    "    for i in range (1,7,2):\n",
    "        U(params, wires = [i, i+1])\n",
    "    \n",
    "def conv_layer2(U, params):\n",
    "    U(params, wires = [0,2])\n",
    "    U(params, wires = [4,6])\n",
    "    U(params, wires = [2,4])\n",
    "    U(params, wires = [0,6])\n",
    "    \n",
    "def pooling_layer1(V_0, V_1, params):\n",
    "    for i in range(0,8,2):\n",
    "        V_0(params[0], wires = [i+1, i])\n",
    "    for i in range(0,8,2):\n",
    "        qml.PauliX(wires = i+1)\n",
    "    for i in range(0,8,2):\n",
    "        V_1(params[1], wires = [i+1, i])\n",
    "        \n",
    "\n",
    "def pooling_layer2(V_0, V_1, params): # 2params\n",
    "    V_0(params[0], wires = [2,0])\n",
    "    V_0(params[0], wires = [6,4])\n",
    "    \n",
    "    qml.PauliX(wires = 2)\n",
    "    qml.PauliX(wires = 6)\n",
    "    \n",
    "    V_1(params[1], wires = [2,0])\n",
    "    V_1(params[1], wires = [6,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define various possible embedding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.templates.embeddings import AmplitudeEmbedding, AngleEmbedding\n",
    "\n",
    "def data_embedding(X, embedding_type = 'Amplitude'):\n",
    "    if embedding_type == 'Amplitude':\n",
    "        AmplitudeEmbedding(X, wires = range(8), normalize = True)\n",
    "    elif embedding_type == 'Angle':\n",
    "        AngleEmbedding(X, wires = range(8), rotation = 'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define QCNN circuit with given Unitary Ansatz and embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires = 8)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def QCNN(X, params, U, U_params, embedding_type = 'Amplitude'):\n",
    "\n",
    "    param1 = params[0:U_params]\n",
    "    param2 = params[U_params:U_params + 2]\n",
    "    param3 = params[U_params + 2: 2*U_params + 2]\n",
    "    param4 = params[2*U_params + 2: 2*U_params + 4]\n",
    "    param5 = params[2*U_params + 4]\n",
    "    \n",
    "    # Data Embedding\n",
    "    data_embedding(X, embedding_type = embedding_type)\n",
    "    \n",
    "    #Quantum Convolutional Neural Network\n",
    "    if U == 'U_TTN':\n",
    "        conv_layer1(U_TTN, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_TTN, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_5':\n",
    "        conv_layer1(U_5, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_5, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_6':\n",
    "        conv_layer1(U_6, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_6, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_9':\n",
    "        conv_layer1(U_9, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_9, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_13':\n",
    "        conv_layer1(U_13, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_13, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_14':\n",
    "        conv_layer1(U_14, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_14, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    elif U == 'U_15':\n",
    "        conv_layer1(U_15, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_15, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "    \n",
    "    elif U == 'U_SO4':\n",
    "        conv_layer1(U_SO4, param1)\n",
    "        pooling_layer1(V_0, V_1, param2)\n",
    "        conv_layer2(U_SO4, param3)\n",
    "        pooling_layer2(V_0, V_1, param4)\n",
    "        F(param5, wires = [0,4])\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Hierarchical Quantum Classifier structure with different Ansatze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_TTN = qml.device('default.qubit', wires = 8)\n",
    "\n",
    "@qml.qnode(dev_TTN)\n",
    "def Hierarchical_classifier(X, params, U, U_params, embedding_type = 'Amplitude'):\n",
    "    \n",
    "    param1 = params[0 * U_params:1 * U_params]\n",
    "    param2 = params[1 * U_params:2 * U_params]\n",
    "    param3 = params[2 * U_params:3 * U_params]\n",
    "    param4 = params[3 * U_params:4 * U_params]\n",
    "    param5 = params[4 * U_params:5 * U_params]\n",
    "    param6 = params[5 * U_params:6 * U_params]\n",
    "    param7 = params[6 * U_params:7 * U_params]\n",
    "    \n",
    "    data_embedding(X, embedding_type = embedding_type)\n",
    "    ['U_TTN', 'U_5', 'U_6', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "    if U == 'U_TTN':\n",
    "        # layer 1\n",
    "        U_TTN(param1, wires = [0,1])\n",
    "        U_TTN(param2, wires = [2,3])\n",
    "        U_TTN(param3, wires = [4,5])\n",
    "        U_TTN(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_TTN(param5, wires = [1,3])\n",
    "        U_TTN(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_TTN(param7, wires = [3,7])\n",
    "    elif U == 'U_5':\n",
    "        # layer 1\n",
    "        U_5(param1, wires = [0,1])\n",
    "        U_5(param2, wires = [2,3])\n",
    "        U_5(param3, wires = [4,5])\n",
    "        U_5(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_5(param5, wires = [1,3])\n",
    "        U_5(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_5(param7, wires = [3,7])\n",
    "    elif U == 'U_6':\n",
    "        # layer 1\n",
    "        U_6(param1, wires = [0,1])\n",
    "        U_6(param2, wires = [2,3])\n",
    "        U_6(param3, wires = [4,5])\n",
    "        U_6(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_6(param5, wires = [1,3])\n",
    "        U_6(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_6(param7, wires = [3,7])\n",
    "    elif U == 'U_13':\n",
    "        # layer 1\n",
    "        U_13(param1, wires = [0,1])\n",
    "        U_13(param2, wires = [2,3])\n",
    "        U_13(param3, wires = [4,5])\n",
    "        U_13(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_13(param5, wires = [1,3])\n",
    "        U_13(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_13(param7, wires = [3,7])\n",
    "    elif U == 'U_14':\n",
    "        # layer 1\n",
    "        U_14(param1, wires = [0,1])\n",
    "        U_14(param2, wires = [2,3])\n",
    "        U_14(param3, wires = [4,5])\n",
    "        U_14(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_14(param5, wires = [1,3])\n",
    "        U_14(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_14(param7, wires = [3,7])\n",
    "    elif U == 'U_15':\n",
    "        # layer 1\n",
    "        U_15(param1, wires = [0,1])\n",
    "        U_15(param2, wires = [2,3])\n",
    "        U_15(param3, wires = [4,5])\n",
    "        U_15(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_15(param5, wires = [1,3])\n",
    "        U_15(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_15(param7, wires = [3,7])\n",
    "    elif U == 'U_SO4':\n",
    "        # layer 1\n",
    "        U_SO4(param1, wires = [0,1])\n",
    "        U_SO4(param2, wires = [2,3])\n",
    "        U_SO4(param3, wires = [4,5])\n",
    "        U_SO4(param4, wires = [6,7])\n",
    "        # layer 2\n",
    "        U_SO4(param5, wires = [1,3])\n",
    "        U_SO4(param6, wires = [5,7])\n",
    "        # layer 3\n",
    "        U_SO4(param7, wires = [3,7])\n",
    "    \n",
    "\n",
    "    return qml.expval(qml.PauliZ(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Quantum Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(params, X, Y, U, U_params, embedding_type, circuit):\n",
    "    if circuit == 'QCNN':\n",
    "        predictions = [QCNN(x, params, U, U_params, embedding_type) for x in X]\n",
    "    elif circuit == 'Hierarchical':\n",
    "        predictions = [Hierarchical_classifier(x, params, U, U_params, embedding_type) for x in X]\n",
    "    \n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "def accuracy_test_binary(predictions, labels):\n",
    "    acc = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 1:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)\n",
    "\n",
    "def accuracy_test_one_class(predictions, labels):\n",
    "    acc = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 0.5:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_training(X_train, Y_train, U, U_params, embedding_type, circuit):\n",
    "    if circuit == 'QCNN':\n",
    "        total_params = U_params * 2 + 2 * 2 + 1\n",
    "    elif circuit == 'Hierarchical':\n",
    "        total_params = U_params * 7\n",
    "        \n",
    "    params = np.random.randn(total_params)\n",
    "    steps = 150\n",
    "    learning_rate = 0.1\n",
    "    batch_size =25\n",
    "    opt = qml.NesterovMomentumOptimizer(learning_rate)\n",
    "    \n",
    "    for it in range(steps):\n",
    "        batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "        X_batch = [X_train[i] for i in batch_index]\n",
    "        Y_batch = [Y_train[i] for i in batch_index]\n",
    "        params, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U, U_params, embedding_type, circuit), params)\n",
    "        if it % 10 == 0:\n",
    "            print(\"iteration: \", it, \" cost: \", cost_new)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Data loading and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA and Autoencoder to reduce it into 8 features. We test both one-class classification (labeling 0 and 1) and binary classification (labeling -1 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "def data_load_and_process(classes = [0,1], feature_reduction = 'resize256', binary = True):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0 #normalize the data\n",
    "    \n",
    "    x_train_filter_01 = np.where((y_train == classes[0]) | (y_train == classes[1]))\n",
    "    x_test_filter_01 = np.where((y_test == classes[0]) | (y_test == classes[1]))\n",
    "    \n",
    "    x_train_01, x_test_01 = x_train[x_train_filter_01], x_test[x_test_filter_01]\n",
    "    y_train_01, y_test_01 = y_train[x_train_filter_01], y_test[x_test_filter_01]\n",
    "    \n",
    "    if binary == False:\n",
    "        y_train_01 = [1 if y ==classes[0] else 0 for y in y_train_01]\n",
    "        y_test_01 = [1 if y ==classes[0] else 0 for y in y_test_01]\n",
    "    elif binary == True:\n",
    "        y_train_01 = [1 if y ==classes[0] else -1 for y in y_train_01]\n",
    "        y_test_01 = [1 if y ==classes[0] else -1 for y in y_test_01]\n",
    "        \n",
    "    \n",
    "    if feature_reduction == 'resize256':   \n",
    "        x_train_01 = tf.image.resize(x_train_01[:], (256, 1)).numpy()\n",
    "        x_test_01 = tf.image.resize(x_test_01[:], (256, 1)).numpy()\n",
    "        x_train_01, x_test_01 = tf.squeeze(x_train_01), tf.squeeze(x_test_01) \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "    \n",
    "    elif feature_reduction == 'pca8':\n",
    "        x_train_01 = tf.image.resize(x_train_01[:], (784, 1)).numpy()\n",
    "        x_test_01 = tf.image.resize(x_test_01[:], (784, 1)).numpy()\n",
    "        x_train_01, x_test_01 = tf.squeeze(x_train_01), tf.squeeze(x_test_01)\n",
    "        \n",
    "        pca = PCA(8)\n",
    "        x_train_01 = pca.fit_transform(x_train_01)\n",
    "        x_test_01 = pca.transform(x_test_01)\n",
    "        \n",
    "        #Rescale for angle embedding\n",
    "        x_train_01, x_test_01 = (x_train_01 + 10) * (np.pi / 20), (x_test_01 + 10) * (np.pi / 20)\n",
    "        \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n",
    "\n",
    "    elif feature_reduction == 'autoencoder8':\n",
    "        latent_dim = 8 \n",
    "        class Autoencoder(Model):\n",
    "            def __init__(self, latent_dim):\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.latent_dim = latent_dim   \n",
    "                self.encoder = tf.keras.Sequential([\n",
    "                layers.Flatten(),\n",
    "                  layers.Dense(latent_dim, activation='relu'),\n",
    "                ])\n",
    "                self.decoder = tf.keras.Sequential([\n",
    "                layers.Dense(784, activation='sigmoid'),\n",
    "                layers.Reshape((28, 28))\n",
    "                ])\n",
    "            def call(self, x):\n",
    "                encoded = self.encoder(x)\n",
    "                decoded = self.decoder(encoded)\n",
    "                return decoded\n",
    "        \n",
    "        autoencoder = Autoencoder(latent_dim)\n",
    "        \n",
    "        autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "        autoencoder.fit(x_train_01, x_train_01,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_01, x_test_01))\n",
    "        \n",
    "        x_train_01, x_test_01 = autoencoder.encoder(x_train_01).numpy(), autoencoder.encoder(x_test_01).numpy()\n",
    "        #Rescale for Angle Embedding\n",
    "        x_train_01, x_test_01 = x_train_01 * (np.pi / 50), x_test_01 * (np.pi / 50)\n",
    "        \n",
    "        return x_train_01, x_test_01, y_train_01, y_test_01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking function trains and performs accuracy tests for all the given unitary ansatze, feature reduction methods, and circuit structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = True):\n",
    "    I = len(Unitaries)\n",
    "    J = len(Encodings)\n",
    "    All_predictions = []\n",
    "\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            U = Unitaries[i]\n",
    "            U_params = U_num_params[i]\n",
    "            Encoding = Encodings[j]\n",
    "            if Encoding == 'resize256':\n",
    "                Embedding = 'Amplitude'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(classes = classes, feature_reduction = 'resize256', binary = binary)\n",
    "            elif Encoding == 'pca8':\n",
    "                Embedding = 'Angle'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(classes = classes, feature_reduction = 'pca8', binary = binary)\n",
    "            elif Encoding == 'autoencoder8':\n",
    "                Embedding = 'Angle'\n",
    "                X_train, X_test, Y_train, Y_test = data_load_and_process(classes = classes, feature_reduction = 'autoencoder8', binary = binary)\n",
    "        \n",
    "            print(\"\\n\")\n",
    "            print(\"Loss History for \" + circuit + \" circuits, \"+ U + \" \" + Encoding)\n",
    "            trained_params = circuit_training(X_train, Y_train, U, U_params, Embedding, circuit)\n",
    "            \n",
    "            if circuit == 'QCNN':\n",
    "                predictions = [QCNN(x, trained_params, U, U_params, Embedding) for x in X_test]\n",
    "            elif circuit == 'Hierarchical':\n",
    "                predictions = [Hierarchical_classifier(x, trained_params, U, U_params, Embedding) for x in X_test]\n",
    "                \n",
    "                \n",
    "            if binary == True:\n",
    "                accuracy = accuracy_test_binary(predictions, Y_test)\n",
    "            elif binary == False:\n",
    "                accuracy = accuracy_test_one_class(predictions, Y_test)\n",
    "                \n",
    "            print(\"Accuracy for \" + U + \" \" + Encoding + \" :\" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Results for QCNN circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Binary Classification with 1, -1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_9', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "U_num_params = [2, 10, 10, 4, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'QCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 One Class Classification with labels 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_9', 'U_13', 'U_14', 'U_15', 'U_SO4']\n",
    "U_num_params = [2, 10, 10, 4, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'QCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results for Hierarchical Classifier circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Binary Classification with 1, -1 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "U_num_params = [2, 10, 10, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'Hierarchical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN resize256\n",
      "iteration:  0  cost:  1.4254236737289714\n",
      "iteration:  10  cost:  0.6964376332391631\n",
      "iteration:  20  cost:  0.385839703052776\n",
      "iteration:  30  cost:  0.6624269072028788\n",
      "iteration:  40  cost:  0.4736939711273403\n",
      "iteration:  50  cost:  0.41880031707818666\n",
      "iteration:  60  cost:  0.5528576465503573\n",
      "iteration:  70  cost:  0.5575690099322155\n",
      "iteration:  80  cost:  0.3721928613545859\n",
      "iteration:  90  cost:  0.32498144777837257\n",
      "iteration:  100  cost:  0.19269116603381953\n",
      "iteration:  110  cost:  0.36746825596288313\n",
      "iteration:  120  cost:  0.3067873718541184\n",
      "iteration:  130  cost:  0.3259691446748894\n",
      "iteration:  140  cost:  0.3607067568495287\n",
      "Accuracy for U_TTN resize256 :0.9697399527186761\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN pca8\n",
      "iteration:  0  cost:  1.0644402758747809\n",
      "iteration:  10  cost:  0.954259410192369\n",
      "iteration:  20  cost:  0.8311068824305157\n",
      "iteration:  30  cost:  0.41639199980415315\n",
      "iteration:  40  cost:  0.4032424950834571\n",
      "iteration:  50  cost:  0.33659524510265326\n",
      "iteration:  60  cost:  0.3527158901359776\n",
      "iteration:  70  cost:  0.5264142891260619\n",
      "iteration:  80  cost:  0.24310212142634932\n",
      "iteration:  90  cost:  0.3551992322603145\n",
      "iteration:  100  cost:  0.2997099298924978\n",
      "iteration:  110  cost:  0.362509306596964\n",
      "iteration:  120  cost:  0.3159519060025655\n",
      "iteration:  130  cost:  0.23889593532974332\n",
      "iteration:  140  cost:  0.4401551406517039\n",
      "Accuracy for U_TTN pca8 :0.9673758865248226\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 63us/sample - loss: 0.0630 - val_loss: 0.0346\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 1s 47us/sample - loss: 0.0300 - val_loss: 0.0266\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 1s 43us/sample - loss: 0.0254 - val_loss: 0.0239\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0230 - val_loss: 0.0216\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0213 - val_loss: 0.0202\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0202 - val_loss: 0.0194\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0194 - val_loss: 0.0189\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 1s 43us/sample - loss: 0.0183 - val_loss: 0.0178\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN autoencoder8\n",
      "iteration:  0  cost:  0.9287996620420949\n",
      "iteration:  10  cost:  0.5839446308103682\n",
      "iteration:  20  cost:  0.4868617056559588\n",
      "iteration:  30  cost:  0.46903295184498345\n",
      "iteration:  40  cost:  0.44547337338514886\n",
      "iteration:  50  cost:  0.4709291304842248\n",
      "iteration:  60  cost:  0.2568436782364198\n",
      "iteration:  70  cost:  0.3363763128664132\n",
      "iteration:  80  cost:  0.2613623383974762\n",
      "iteration:  90  cost:  0.36491087001478767\n",
      "iteration:  100  cost:  0.4185086391128328\n",
      "iteration:  110  cost:  0.4105554166711026\n",
      "iteration:  120  cost:  0.4367245350130077\n",
      "iteration:  130  cost:  0.4112624294409452\n",
      "iteration:  140  cost:  0.3000756431729917\n",
      "Accuracy for U_TTN autoencoder8 :0.9144208037825059\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 resize256\n",
      "iteration:  0  cost:  1.0093771219979053\n",
      "iteration:  10  cost:  0.9999621853491196\n",
      "iteration:  20  cost:  1.0101142400923957\n",
      "iteration:  30  cost:  0.9972236499720948\n",
      "iteration:  40  cost:  0.9962190671751674\n",
      "iteration:  50  cost:  1.015370873246477\n",
      "iteration:  60  cost:  1.0106136567040545\n",
      "iteration:  70  cost:  0.9951824579780529\n",
      "iteration:  80  cost:  1.0092605494845808\n",
      "iteration:  90  cost:  0.9855415120945541\n",
      "iteration:  100  cost:  1.0507812081192014\n",
      "iteration:  110  cost:  0.965828144805441\n",
      "iteration:  120  cost:  0.9281932724535629\n",
      "iteration:  130  cost:  0.8543836580644928\n",
      "iteration:  140  cost:  0.4158847587850478\n",
      "Accuracy for U_5 resize256 :0.9498817966903074\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 pca8\n",
      "iteration:  0  cost:  1.3906287801043589\n",
      "iteration:  10  cost:  1.0211024961348663\n",
      "iteration:  20  cost:  0.9203878841218237\n",
      "iteration:  30  cost:  0.8867811162475894\n",
      "iteration:  40  cost:  0.9638641842135783\n",
      "iteration:  50  cost:  1.0488784042935986\n",
      "iteration:  60  cost:  0.9433586011965568\n",
      "iteration:  70  cost:  0.8842703613926527\n",
      "iteration:  80  cost:  0.9420923051710125\n",
      "iteration:  90  cost:  1.0815163847201417\n",
      "iteration:  100  cost:  0.9785695652751495\n",
      "iteration:  110  cost:  0.9849375638474196\n",
      "iteration:  120  cost:  0.9705406266519191\n",
      "iteration:  130  cost:  0.9481045588856252\n",
      "iteration:  140  cost:  0.9803122134826893\n",
      "Accuracy for U_5 pca8 :0.607565011820331\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 58us/sample - loss: 0.0640 - val_loss: 0.0335\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0296 - val_loss: 0.0268\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0257 - val_loss: 0.0239\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0232 - val_loss: 0.0218\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0213 - val_loss: 0.0201\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0191 - val_loss: 0.0184\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0182 - val_loss: 0.0177\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0179 - val_loss: 0.0174\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 autoencoder8\n",
      "iteration:  0  cost:  1.5728436140881967\n",
      "iteration:  10  cost:  0.7661743282855591\n",
      "iteration:  20  cost:  0.38780790130099535\n",
      "iteration:  30  cost:  0.33830022334738397\n",
      "iteration:  40  cost:  0.3539911278018992\n",
      "iteration:  50  cost:  0.45580262441743374\n",
      "iteration:  60  cost:  0.37292889336763885\n",
      "iteration:  70  cost:  0.37338094469555505\n",
      "iteration:  80  cost:  0.23451038056745804\n",
      "iteration:  90  cost:  0.44229134524604924\n",
      "iteration:  100  cost:  0.3310444440364451\n",
      "iteration:  110  cost:  0.3337666490473295\n",
      "iteration:  120  cost:  0.4095344222263811\n",
      "iteration:  130  cost:  0.47124319364785644\n",
      "iteration:  140  cost:  0.25162790061853535\n",
      "Accuracy for U_5 autoencoder8 :0.9598108747044918\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 resize256\n",
      "iteration:  0  cost:  1.642112060648264\n",
      "iteration:  10  cost:  0.830944947464337\n",
      "iteration:  20  cost:  0.4198823586796053\n",
      "iteration:  30  cost:  0.31097897935061325\n",
      "iteration:  40  cost:  0.273805291864505\n",
      "iteration:  50  cost:  0.25068182552024454\n",
      "iteration:  60  cost:  0.2032409540916389\n",
      "iteration:  70  cost:  0.3612946384432894\n",
      "iteration:  80  cost:  0.29120402034576093\n",
      "iteration:  90  cost:  0.28741107442780933\n",
      "iteration:  100  cost:  0.19972743503350301\n",
      "iteration:  110  cost:  0.21811016734958244\n",
      "iteration:  120  cost:  0.22859818014145034\n",
      "iteration:  130  cost:  0.2197853438196365\n",
      "iteration:  140  cost:  0.18325221009825476\n",
      "Accuracy for U_6 resize256 :0.9881796690307328\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 pca8\n",
      "iteration:  0  cost:  1.2318987246390194\n",
      "iteration:  10  cost:  0.893759887787355\n",
      "iteration:  20  cost:  0.48739012455523506\n",
      "iteration:  30  cost:  0.15252969140875694\n",
      "iteration:  40  cost:  0.1670787512956447\n",
      "iteration:  50  cost:  0.12204676608108184\n",
      "iteration:  60  cost:  0.10316412723273642\n",
      "iteration:  70  cost:  0.10180058747353023\n",
      "iteration:  80  cost:  0.18540051955886347\n",
      "iteration:  90  cost:  0.16512304671106645\n",
      "iteration:  100  cost:  0.23621781091725583\n",
      "iteration:  110  cost:  0.08832496889944798\n",
      "iteration:  120  cost:  0.07377696188242516\n",
      "iteration:  130  cost:  0.12058441147884288\n",
      "iteration:  140  cost:  0.09621155876964366\n",
      "Accuracy for U_6 pca8 :0.9810874704491725\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 58us/sample - loss: 0.0659 - val_loss: 0.0351\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0307 - val_loss: 0.0276\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0258 - val_loss: 0.0237\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0231 - val_loss: 0.0220\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0216 - val_loss: 0.0208\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0204 - val_loss: 0.0198\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 1s 40us/sample - loss: 0.0196 - val_loss: 0.0191\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 1s 40us/sample - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 39us/sample - loss: 0.0182 - val_loss: 0.0178\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 autoencoder8\n",
      "iteration:  0  cost:  1.3754560690393705\n",
      "iteration:  10  cost:  0.6464584820362789\n",
      "iteration:  20  cost:  0.5649154955686752\n",
      "iteration:  30  cost:  0.5019799872123866\n",
      "iteration:  40  cost:  0.36064098424915586\n",
      "iteration:  50  cost:  0.4542221617371231\n",
      "iteration:  60  cost:  0.32072575976157275\n",
      "iteration:  70  cost:  0.35957296341241735\n",
      "iteration:  80  cost:  0.30040514622223724\n",
      "iteration:  90  cost:  0.32283921094258683\n",
      "iteration:  100  cost:  0.43120963039949267\n",
      "iteration:  110  cost:  0.23528874838793534\n",
      "iteration:  120  cost:  0.31886905064124244\n",
      "iteration:  130  cost:  0.34876407296936207\n",
      "iteration:  140  cost:  0.22463171286070605\n",
      "Accuracy for U_6 autoencoder8 :0.9702127659574468\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 resize256\n",
      "iteration:  0  cost:  1.0095500199148835\n",
      "iteration:  10  cost:  0.8831846967537993\n",
      "iteration:  20  cost:  0.903328479086914\n",
      "iteration:  30  cost:  0.7846645693360366\n",
      "iteration:  40  cost:  0.5521773682424592\n",
      "iteration:  50  cost:  0.6588026342809402\n",
      "iteration:  60  cost:  0.5341613239688354\n",
      "iteration:  70  cost:  0.31959299857925394\n",
      "iteration:  80  cost:  0.42579977030873584\n",
      "iteration:  90  cost:  0.27532759582287725\n",
      "iteration:  100  cost:  0.4009205363561025\n",
      "iteration:  110  cost:  0.3345445190308707\n",
      "iteration:  120  cost:  0.3738860642469315\n",
      "iteration:  130  cost:  0.2764267287168043\n",
      "iteration:  140  cost:  0.29560832108512214\n",
      "Accuracy for U_13 resize256 :0.9810874704491725\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 pca8\n",
      "iteration:  0  cost:  1.5200627844390449\n",
      "iteration:  10  cost:  1.224910180482327\n",
      "iteration:  20  cost:  1.1232590345746465\n",
      "iteration:  30  cost:  1.2701136792415744\n",
      "iteration:  40  cost:  1.1214545814068115\n",
      "iteration:  50  cost:  1.0774812763940966\n",
      "iteration:  60  cost:  0.9659554065084737\n",
      "iteration:  70  cost:  1.0531933260910757\n",
      "iteration:  80  cost:  0.9598905777408616\n",
      "iteration:  90  cost:  0.9952311796754714\n",
      "iteration:  100  cost:  0.942050332113506\n",
      "iteration:  110  cost:  1.050782392047954\n",
      "iteration:  120  cost:  0.9739117780339457\n",
      "iteration:  130  cost:  1.1059794470083248\n",
      "iteration:  140  cost:  0.9735656337893376\n",
      "Accuracy for U_13 pca8 :0.5352245862884161\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0646 - val_loss: 0.0357\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0305 - val_loss: 0.0277\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0240 - val_loss: 0.0224\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0221 - val_loss: 0.0209\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0209 - val_loss: 0.0200\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0194 - val_loss: 0.0186\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0189 - val_loss: 0.0182\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0185 - val_loss: 0.0179\n",
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 autoencoder8\n",
      "iteration:  0  cost:  1.4632917739452835\n",
      "iteration:  10  cost:  0.29929981840354825\n",
      "iteration:  20  cost:  0.5469514549466767\n",
      "iteration:  30  cost:  0.335821192700997\n",
      "iteration:  40  cost:  0.23939659564630486\n",
      "iteration:  50  cost:  0.28516063536009534\n",
      "iteration:  60  cost:  0.2556064202182402\n",
      "iteration:  70  cost:  0.2278944886292513\n",
      "iteration:  80  cost:  0.265092110771368\n",
      "iteration:  90  cost:  0.19283482461636797\n",
      "iteration:  100  cost:  0.11895613279894109\n",
      "iteration:  110  cost:  0.3866008791300936\n",
      "iteration:  120  cost:  0.1988181032923309\n",
      "iteration:  130  cost:  0.3816335159404091\n",
      "iteration:  140  cost:  0.2256687580328763\n",
      "Accuracy for U_13 autoencoder8 :0.9735224586288416\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 resize256\n",
      "iteration:  0  cost:  1.2087775954388784\n",
      "iteration:  10  cost:  1.0020177103771761\n",
      "iteration:  20  cost:  0.76886754232775\n",
      "iteration:  30  cost:  0.5469748689242703\n",
      "iteration:  40  cost:  0.8063060369354536\n",
      "iteration:  50  cost:  0.5198198620957409\n",
      "iteration:  60  cost:  0.30726973681883\n",
      "iteration:  70  cost:  0.31872559048260746\n",
      "iteration:  80  cost:  0.31242487068179536\n",
      "iteration:  90  cost:  0.31822486775844316\n",
      "iteration:  100  cost:  0.308745707001063\n",
      "iteration:  110  cost:  0.238986304417563\n",
      "iteration:  120  cost:  0.34678945404000594\n",
      "iteration:  130  cost:  0.29633991586213576\n",
      "iteration:  140  cost:  0.2276859680353323\n",
      "Accuracy for U_14 resize256 :0.9739952718676123\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 pca8\n",
      "iteration:  0  cost:  1.075441982318201\n",
      "iteration:  10  cost:  1.5927571408559853\n",
      "iteration:  20  cost:  1.0006120056261605\n",
      "iteration:  30  cost:  1.0262151128094394\n",
      "iteration:  40  cost:  0.9804119695681641\n",
      "iteration:  50  cost:  0.7975374382132128\n",
      "iteration:  60  cost:  0.8613385261496019\n",
      "iteration:  70  cost:  0.7475302218868126\n",
      "iteration:  80  cost:  1.0275217771324392\n",
      "iteration:  90  cost:  0.8313460896890976\n",
      "iteration:  100  cost:  0.5241625871030205\n",
      "iteration:  110  cost:  0.2478769486723725\n",
      "iteration:  120  cost:  0.14139961691508748\n",
      "iteration:  130  cost:  0.11847329270975798\n",
      "iteration:  140  cost:  0.1919886487787657\n",
      "Accuracy for U_14 pca8 :0.9725768321513002\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0647 - val_loss: 0.0365\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0305 - val_loss: 0.0273\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0261 - val_loss: 0.0245\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0234 - val_loss: 0.0221\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0216 - val_loss: 0.0206\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0205 - val_loss: 0.0198\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0192 - val_loss: 0.0186\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0184 - val_loss: 0.0179\n",
      "WARNING:tensorflow:Layer flatten_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 autoencoder8\n",
      "iteration:  0  cost:  1.0531902628499\n",
      "iteration:  10  cost:  0.653871344919674\n",
      "iteration:  20  cost:  0.4549694961656165\n",
      "iteration:  30  cost:  0.3529477730389191\n",
      "iteration:  40  cost:  0.3446139090930156\n",
      "iteration:  50  cost:  0.37568830368883327\n",
      "iteration:  60  cost:  0.45092270628133313\n",
      "iteration:  70  cost:  0.3003136907810043\n",
      "iteration:  80  cost:  0.4319799455267038\n",
      "iteration:  90  cost:  0.30556104188015093\n",
      "iteration:  100  cost:  0.4502572878125459\n",
      "iteration:  110  cost:  0.313153314063631\n",
      "iteration:  120  cost:  0.29198377502465633\n",
      "iteration:  130  cost:  0.2793343526724168\n",
      "iteration:  140  cost:  0.38240871633668616\n",
      "Accuracy for U_14 autoencoder8 :0.9574468085106383\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 resize256\n",
      "iteration:  0  cost:  1.2208044008485568\n",
      "iteration:  10  cost:  0.7338955050518049\n",
      "iteration:  20  cost:  0.4108902924073049\n",
      "iteration:  30  cost:  0.38035968692524025\n",
      "iteration:  40  cost:  0.3055736640864249\n",
      "iteration:  50  cost:  0.19874990952089797\n",
      "iteration:  60  cost:  0.29508243429048564\n",
      "iteration:  70  cost:  0.2676052552543258\n",
      "iteration:  80  cost:  0.1586086331068434\n",
      "iteration:  90  cost:  0.133586347195046\n",
      "iteration:  100  cost:  0.17563368236164062\n",
      "iteration:  110  cost:  0.12297734812937457\n",
      "iteration:  120  cost:  0.19525467223128043\n",
      "iteration:  130  cost:  0.18876052940122418\n",
      "iteration:  140  cost:  0.20941992313282953\n",
      "Accuracy for U_15 resize256 :0.9886524822695035\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 pca8\n",
      "iteration:  0  cost:  0.8969362228468166\n",
      "iteration:  10  cost:  0.7090744478859824\n",
      "iteration:  20  cost:  0.2086290971854179\n",
      "iteration:  30  cost:  0.12410105943020748\n",
      "iteration:  40  cost:  0.16535476125462187\n",
      "iteration:  50  cost:  0.09817227081322652\n",
      "iteration:  60  cost:  0.12510393699513428\n",
      "iteration:  70  cost:  0.10263986732237285\n",
      "iteration:  80  cost:  0.12661510241609877\n",
      "iteration:  90  cost:  0.17265930508392136\n",
      "iteration:  100  cost:  0.08157962130498897\n",
      "iteration:  110  cost:  0.14573758673911924\n",
      "iteration:  120  cost:  0.11238479779190222\n",
      "iteration:  130  cost:  0.12871443808135455\n",
      "iteration:  140  cost:  0.2536947999623079\n",
      "Accuracy for U_15 pca8 :0.9853427895981087\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0655 - val_loss: 0.0365\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0312 - val_loss: 0.0277\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0263 - val_loss: 0.0243\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0232 - val_loss: 0.0219\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0215 - val_loss: 0.0206\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0204 - val_loss: 0.0197\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0195 - val_loss: 0.0191\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0186 - val_loss: 0.0182\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0182 - val_loss: 0.0178\n",
      "WARNING:tensorflow:Layer flatten_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 autoencoder8\n",
      "iteration:  0  cost:  0.8872651859536691\n",
      "iteration:  10  cost:  0.4147219425497003\n",
      "iteration:  20  cost:  0.2935554253485046\n",
      "iteration:  30  cost:  0.23305305478436045\n",
      "iteration:  40  cost:  0.2814739923279155\n",
      "iteration:  50  cost:  0.19573227856001085\n",
      "iteration:  60  cost:  0.25801602015058756\n",
      "iteration:  70  cost:  0.2324566433532428\n",
      "iteration:  80  cost:  0.21113197789812355\n",
      "iteration:  90  cost:  0.21007567320295514\n",
      "iteration:  100  cost:  0.13864058359296139\n",
      "iteration:  110  cost:  0.199714917508006\n",
      "iteration:  120  cost:  0.28455062418058696\n",
      "iteration:  130  cost:  0.2291350930501233\n",
      "iteration:  140  cost:  0.2884806983329339\n",
      "Accuracy for U_15 autoencoder8 :0.9626477541371158\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 resize256\n",
      "iteration:  0  cost:  1.0171822854323356\n",
      "iteration:  10  cost:  0.8298700202520206\n",
      "iteration:  20  cost:  0.3519340983544786\n",
      "iteration:  30  cost:  0.3380694349477043\n",
      "iteration:  40  cost:  0.41665299872201755\n",
      "iteration:  50  cost:  0.40897644180343173\n",
      "iteration:  60  cost:  0.23193505436186782\n",
      "iteration:  70  cost:  0.4086691916862781\n",
      "iteration:  80  cost:  0.28735864382208115\n",
      "iteration:  90  cost:  0.2340530375404646\n",
      "iteration:  100  cost:  0.2173756712878054\n",
      "iteration:  110  cost:  0.29867532122047\n",
      "iteration:  120  cost:  0.27943913358074934\n",
      "iteration:  130  cost:  0.20562540053565023\n",
      "iteration:  140  cost:  0.25462459638188306\n",
      "Accuracy for U_SO4 resize256 :0.9749408983451536\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 pca8\n",
      "iteration:  0  cost:  1.1343081068788687\n",
      "iteration:  10  cost:  0.9137612482610156\n",
      "iteration:  20  cost:  1.062762575811279\n",
      "iteration:  30  cost:  1.0328377949003293\n",
      "iteration:  40  cost:  1.0177894615357794\n",
      "iteration:  50  cost:  0.8979056859370842\n",
      "iteration:  60  cost:  0.29408961542773715\n",
      "iteration:  70  cost:  0.24396744418894148\n",
      "iteration:  80  cost:  0.16067916793574566\n",
      "iteration:  90  cost:  0.09352293656834021\n",
      "iteration:  100  cost:  0.04870044054521038\n",
      "iteration:  110  cost:  0.13586860443352855\n",
      "iteration:  120  cost:  0.13958953373067476\n",
      "iteration:  130  cost:  0.0917622536218489\n",
      "iteration:  140  cost:  0.10391926137634126\n",
      "Accuracy for U_SO4 pca8 :0.9872340425531915\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0664 - val_loss: 0.0353\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0299 - val_loss: 0.0267\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0252 - val_loss: 0.0233\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0224 - val_loss: 0.0210\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0208 - val_loss: 0.0199\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 1s 40us/sample - loss: 0.0198 - val_loss: 0.0191\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0192 - val_loss: 0.0185\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0183 - val_loss: 0.0177\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0181 - val_loss: 0.0175\n",
      "WARNING:tensorflow:Layer flatten_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 autoencoder8\n",
      "iteration:  0  cost:  1.1874181132762482\n",
      "iteration:  10  cost:  0.6734521533917834\n",
      "iteration:  20  cost:  0.16991175271142292\n",
      "iteration:  30  cost:  0.24381507497617305\n",
      "iteration:  40  cost:  0.2902567195381991\n",
      "iteration:  50  cost:  0.13763112489834425\n",
      "iteration:  60  cost:  0.503709015727615\n",
      "iteration:  70  cost:  0.2172640387428687\n",
      "iteration:  80  cost:  0.2533184918919411\n",
      "iteration:  90  cost:  0.4198843568834553\n",
      "iteration:  100  cost:  0.1641041159778238\n",
      "iteration:  110  cost:  0.14945240105600627\n",
      "iteration:  120  cost:  0.262197695098777\n",
      "iteration:  130  cost:  0.22213564298954314\n",
      "iteration:  140  cost:  0.35904066271442786\n",
      "Accuracy for U_SO4 autoencoder8 :0.9536643026004729\n"
     ]
    }
   ],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 One Class Classification with 0, 1 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unitaries = ['U_TTN', 'U_5', 'U_6', 'U_13', 'U_14', 'U_15', 'U_SO4'] \n",
    "U_num_params = [2, 10, 10, 6, 6, 4, 6]\n",
    "Encodings = ['resize256', 'pca8', 'autoencoder8']\n",
    "classes = [0,1]\n",
    "circuit = 'Hierarchical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN resize256\n",
      "iteration:  0  cost:  0.35525773742240896\n",
      "iteration:  10  cost:  0.3328691558220538\n",
      "iteration:  20  cost:  0.2140754008832847\n",
      "iteration:  30  cost:  0.1015911831206335\n",
      "iteration:  40  cost:  0.08523465681568673\n",
      "iteration:  50  cost:  0.13504519589169214\n",
      "iteration:  60  cost:  0.16959733491187542\n",
      "iteration:  70  cost:  0.2056834277536673\n",
      "iteration:  80  cost:  0.10870341952284272\n",
      "iteration:  90  cost:  0.11761417128397657\n",
      "iteration:  100  cost:  0.11893951068590494\n",
      "iteration:  110  cost:  0.09069035817993487\n",
      "iteration:  120  cost:  0.13341179349553453\n",
      "iteration:  130  cost:  0.08403569285131886\n",
      "iteration:  140  cost:  0.10402214866940401\n",
      "Accuracy for U_TTN resize256 :0.9234042553191489\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN pca8\n",
      "iteration:  0  cost:  0.4030474611440642\n",
      "iteration:  10  cost:  0.2398302903707779\n",
      "iteration:  20  cost:  0.19140073583434347\n",
      "iteration:  30  cost:  0.2604377151220624\n",
      "iteration:  40  cost:  0.17877926972908018\n",
      "iteration:  50  cost:  0.1238124106518014\n",
      "iteration:  60  cost:  0.18034799500328547\n",
      "iteration:  70  cost:  0.15470900330100978\n",
      "iteration:  80  cost:  0.1786454403290714\n",
      "iteration:  90  cost:  0.15552341338609912\n",
      "iteration:  100  cost:  0.1080365312462175\n",
      "iteration:  110  cost:  0.13626972451180194\n",
      "iteration:  120  cost:  0.07387872615792274\n",
      "iteration:  130  cost:  0.0590703888519139\n",
      "iteration:  140  cost:  0.1647587794761228\n",
      "Accuracy for U_TTN pca8 :0.9408983451536643\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 59us/sample - loss: 0.0638 - val_loss: 0.0336\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 1s 44us/sample - loss: 0.0290 - val_loss: 0.0264\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 1s 44us/sample - loss: 0.0250 - val_loss: 0.0233\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 1s 40us/sample - loss: 0.0223 - val_loss: 0.0212\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0206 - val_loss: 0.0198\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0191 - val_loss: 0.0187\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 1s 42us/sample - loss: 0.0187 - val_loss: 0.0183\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0184 - val_loss: 0.0180\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 1s 41us/sample - loss: 0.0181 - val_loss: 0.0178\n",
      "WARNING:tensorflow:Layer flatten_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_TTN autoencoder8\n",
      "iteration:  0  cost:  0.45104668114502133\n",
      "iteration:  10  cost:  0.13006874763709894\n",
      "iteration:  20  cost:  0.16286208703665153\n",
      "iteration:  30  cost:  0.06777652117910357\n",
      "iteration:  40  cost:  0.0459391601608008\n",
      "iteration:  50  cost:  0.06394366327047958\n",
      "iteration:  60  cost:  0.03721093113572069\n",
      "iteration:  70  cost:  0.034127685274360886\n",
      "iteration:  80  cost:  0.08091913406808816\n",
      "iteration:  90  cost:  0.05133387049661154\n",
      "iteration:  100  cost:  0.039230947766204144\n",
      "iteration:  110  cost:  0.07935346013945002\n",
      "iteration:  120  cost:  0.05189745380239168\n",
      "iteration:  130  cost:  0.05803668770364166\n",
      "iteration:  140  cost:  0.03936430964616579\n",
      "Accuracy for U_TTN autoencoder8 :0.9687943262411347\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 resize256\n",
      "iteration:  0  cost:  0.997476893650433\n",
      "iteration:  10  cost:  0.4954167402847351\n",
      "iteration:  20  cost:  0.24272056847158224\n",
      "iteration:  30  cost:  0.20005502694858276\n",
      "iteration:  40  cost:  0.13455155990291945\n",
      "iteration:  50  cost:  0.062391772882405515\n",
      "iteration:  60  cost:  0.06782984679061964\n",
      "iteration:  70  cost:  0.051669669560627914\n",
      "iteration:  80  cost:  0.08555047077084968\n",
      "iteration:  90  cost:  0.07754165842500776\n",
      "iteration:  100  cost:  0.08747648131396935\n",
      "iteration:  110  cost:  0.08216842735858754\n",
      "iteration:  120  cost:  0.06763290308263083\n",
      "iteration:  130  cost:  0.057370274365762225\n",
      "iteration:  140  cost:  0.04137957185042605\n",
      "Accuracy for U_5 resize256 :0.9730496453900709\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 pca8\n",
      "iteration:  0  cost:  0.4467016707658158\n",
      "iteration:  10  cost:  0.24165864807435697\n",
      "iteration:  20  cost:  0.39272876596729467\n",
      "iteration:  30  cost:  0.23641842619271533\n",
      "iteration:  40  cost:  0.2620000985371808\n",
      "iteration:  50  cost:  0.2583071047066748\n",
      "iteration:  60  cost:  0.2529738061437965\n",
      "iteration:  70  cost:  0.2663020601518469\n",
      "iteration:  80  cost:  0.23204714303962157\n",
      "iteration:  90  cost:  0.197878813750475\n",
      "iteration:  100  cost:  0.10714220263455583\n",
      "iteration:  110  cost:  0.08582382208494688\n",
      "iteration:  120  cost:  0.05874817954019195\n",
      "iteration:  130  cost:  0.030301907481934953\n",
      "iteration:  140  cost:  0.06545392653193942\n",
      "Accuracy for U_5 pca8 :0.9527186761229315\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0660 - val_loss: 0.0370\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0316 - val_loss: 0.0282\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0271 - val_loss: 0.0257\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0247 - val_loss: 0.0235\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0230 - val_loss: 0.0223\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0221 - val_loss: 0.0215\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0215 - val_loss: 0.0210\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0211 - val_loss: 0.0206\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0207 - val_loss: 0.0203\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0204 - val_loss: 0.0201\n",
      "WARNING:tensorflow:Layer flatten_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_5 autoencoder8\n",
      "iteration:  0  cost:  0.792473784379411\n",
      "iteration:  10  cost:  0.14829486428336727\n",
      "iteration:  20  cost:  0.09960659126649281\n",
      "iteration:  30  cost:  0.07587366534578206\n",
      "iteration:  40  cost:  0.13129787976785048\n",
      "iteration:  50  cost:  0.170075390009835\n",
      "iteration:  60  cost:  0.25372058195333985\n",
      "iteration:  70  cost:  0.08029172844577716\n",
      "iteration:  80  cost:  0.133766905830799\n",
      "iteration:  90  cost:  0.11004888054169273\n",
      "iteration:  100  cost:  0.1343455148924195\n",
      "iteration:  110  cost:  0.08182072952662356\n",
      "iteration:  120  cost:  0.0937487055403106\n",
      "iteration:  130  cost:  0.1531691639080851\n",
      "iteration:  140  cost:  0.15391712490631615\n",
      "Accuracy for U_5 autoencoder8 :0.8770685579196218\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 resize256\n",
      "iteration:  0  cost:  0.28987397936802606\n",
      "iteration:  10  cost:  0.24476557047361036\n",
      "iteration:  20  cost:  0.221873622868841\n",
      "iteration:  30  cost:  0.20406692830207832\n",
      "iteration:  40  cost:  0.18201165271213554\n",
      "iteration:  50  cost:  0.13390876141313418\n",
      "iteration:  60  cost:  0.11024532470327837\n",
      "iteration:  70  cost:  0.11863987837480665\n",
      "iteration:  80  cost:  0.08713503682474431\n",
      "iteration:  90  cost:  0.057092328364426674\n",
      "iteration:  100  cost:  0.06691115167054655\n",
      "iteration:  110  cost:  0.09612967182826626\n",
      "iteration:  120  cost:  0.07165359975980291\n",
      "iteration:  130  cost:  0.05380625170591584\n",
      "iteration:  140  cost:  0.028909672664901673\n",
      "Accuracy for U_6 resize256 :0.9640661938534278\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 pca8\n",
      "iteration:  0  cost:  0.179825476662808\n",
      "iteration:  10  cost:  0.24529366735078859\n",
      "iteration:  20  cost:  0.24638989114405582\n",
      "iteration:  30  cost:  0.2314893040082261\n",
      "iteration:  40  cost:  0.3281841540374615\n",
      "iteration:  50  cost:  0.25846907919762097\n",
      "iteration:  60  cost:  0.272708029434377\n",
      "iteration:  70  cost:  0.24049131956690017\n",
      "iteration:  80  cost:  0.20766768869240693\n",
      "iteration:  90  cost:  0.24200552260888084\n",
      "iteration:  100  cost:  0.2340803531583707\n",
      "iteration:  110  cost:  0.23381665791930017\n",
      "iteration:  120  cost:  0.2765202299678139\n",
      "iteration:  130  cost:  0.22252396161097468\n",
      "iteration:  140  cost:  0.2155700355224469\n",
      "Accuracy for U_6 pca8 :0.6260047281323877\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0637 - val_loss: 0.0357\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0302 - val_loss: 0.0269\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0255 - val_loss: 0.0236\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0227 - val_loss: 0.0213\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0208 - val_loss: 0.0197\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0190 - val_loss: 0.0183\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0182 - val_loss: 0.0177\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0180 - val_loss: 0.0174\n",
      "WARNING:tensorflow:Layer flatten_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_6 autoencoder8\n",
      "iteration:  0  cost:  0.884746280387618\n",
      "iteration:  10  cost:  0.18848997964692127\n",
      "iteration:  20  cost:  0.11011102315239783\n",
      "iteration:  30  cost:  0.12921555386963385\n",
      "iteration:  40  cost:  0.07676507491372274\n",
      "iteration:  50  cost:  0.036805043310308844\n",
      "iteration:  60  cost:  0.0342886036101641\n",
      "iteration:  70  cost:  0.025840610665159535\n",
      "iteration:  80  cost:  0.030302511917205947\n",
      "iteration:  90  cost:  0.0294368864741148\n",
      "iteration:  100  cost:  0.022364020828409742\n",
      "iteration:  110  cost:  0.03520522884299213\n",
      "iteration:  120  cost:  0.02540795844764764\n",
      "iteration:  130  cost:  0.009567073911783198\n",
      "iteration:  140  cost:  0.02170171892454082\n",
      "Accuracy for U_6 autoencoder8 :0.9895981087470449\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 resize256\n",
      "iteration:  0  cost:  1.0423310682545885\n",
      "iteration:  10  cost:  0.2036813482775628\n",
      "iteration:  20  cost:  0.14925755665068968\n",
      "iteration:  30  cost:  0.09303801288887564\n",
      "iteration:  40  cost:  0.09605731116415478\n",
      "iteration:  50  cost:  0.10351786512725802\n",
      "iteration:  60  cost:  0.12245715285192763\n",
      "iteration:  70  cost:  0.10106700074236845\n",
      "iteration:  80  cost:  0.12204232281301886\n",
      "iteration:  90  cost:  0.07702222687175238\n",
      "iteration:  100  cost:  0.06224875462532641\n",
      "iteration:  110  cost:  0.09364121016156156\n",
      "iteration:  120  cost:  0.09424924464472799\n",
      "iteration:  130  cost:  0.07298270730124014\n",
      "iteration:  140  cost:  0.06909019698559442\n",
      "Accuracy for U_13 resize256 :0.9569739952718677\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 pca8\n",
      "iteration:  0  cost:  0.43173955057230373\n",
      "iteration:  10  cost:  0.272961987794069\n",
      "iteration:  20  cost:  0.23052119190545922\n",
      "iteration:  30  cost:  0.25442088269563656\n",
      "iteration:  40  cost:  0.2732107854157789\n",
      "iteration:  50  cost:  0.2616558750291943\n",
      "iteration:  60  cost:  0.2522650496231968\n",
      "iteration:  70  cost:  0.2733007920675225\n",
      "iteration:  80  cost:  0.2662582099380435\n",
      "iteration:  90  cost:  0.2638462356152719\n",
      "iteration:  100  cost:  0.24825066046426592\n",
      "iteration:  110  cost:  0.24733779824232321\n",
      "iteration:  120  cost:  0.2861215374859574\n",
      "iteration:  130  cost:  0.23848791920717619\n",
      "iteration:  140  cost:  0.22278492578654205\n",
      "Accuracy for U_13 pca8 :0.5858156028368794\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0645 - val_loss: 0.0354\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0309 - val_loss: 0.0275\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0263 - val_loss: 0.0243\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0233 - val_loss: 0.0216\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0211 - val_loss: 0.0200\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0193 - val_loss: 0.0186\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0188 - val_loss: 0.0183\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0183 - val_loss: 0.0177\n",
      "WARNING:tensorflow:Layer flatten_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_13 autoencoder8\n",
      "iteration:  0  cost:  0.23734756706186622\n",
      "iteration:  10  cost:  0.23610652127373524\n",
      "iteration:  20  cost:  0.24084124366331927\n",
      "iteration:  30  cost:  0.17347289183362954\n",
      "iteration:  40  cost:  0.15365096011179236\n",
      "iteration:  50  cost:  0.11987188929010711\n",
      "iteration:  60  cost:  0.14831156030235199\n",
      "iteration:  70  cost:  0.13736283249631942\n",
      "iteration:  80  cost:  0.12923184966949913\n",
      "iteration:  90  cost:  0.10580906657195614\n",
      "iteration:  100  cost:  0.11944241277138326\n",
      "iteration:  110  cost:  0.09911839661842714\n",
      "iteration:  120  cost:  0.14159287274109245\n",
      "iteration:  130  cost:  0.06880974627480627\n",
      "iteration:  140  cost:  0.12777236721733445\n",
      "Accuracy for U_13 autoencoder8 :0.893144208037825\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 resize256\n",
      "iteration:  0  cost:  1.7194838363704903\n",
      "iteration:  10  cost:  0.2628228495833312\n",
      "iteration:  20  cost:  0.2628164758677851\n",
      "iteration:  30  cost:  0.23258569953932234\n",
      "iteration:  40  cost:  0.2560687476009314\n",
      "iteration:  50  cost:  0.25187713990138\n",
      "iteration:  60  cost:  0.2322332966877765\n",
      "iteration:  70  cost:  0.2288006626769609\n",
      "iteration:  80  cost:  0.19166566407852803\n",
      "iteration:  90  cost:  0.17342371838088788\n",
      "iteration:  100  cost:  0.14985685741619306\n",
      "iteration:  110  cost:  0.10084736438631937\n",
      "iteration:  120  cost:  0.07206221661324176\n",
      "iteration:  130  cost:  0.0444698034671169\n",
      "iteration:  140  cost:  0.06096957504932561\n",
      "Accuracy for U_14 resize256 :0.950354609929078\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 pca8\n",
      "iteration:  0  cost:  0.3713248396848535\n",
      "iteration:  10  cost:  0.3014093230569053\n",
      "iteration:  20  cost:  0.24735052101282895\n",
      "iteration:  30  cost:  0.24911719884334263\n",
      "iteration:  40  cost:  0.2599090729518148\n",
      "iteration:  50  cost:  0.24193862439651903\n",
      "iteration:  60  cost:  0.21153276536061608\n",
      "iteration:  70  cost:  0.23669365346708285\n",
      "iteration:  80  cost:  0.2374058514271298\n",
      "iteration:  90  cost:  0.1939281876582476\n",
      "iteration:  100  cost:  0.13752384151718114\n",
      "iteration:  110  cost:  0.09667717082752501\n",
      "iteration:  120  cost:  0.0840301148830599\n",
      "iteration:  130  cost:  0.0342483832437462\n",
      "iteration:  140  cost:  0.038693163594429134\n",
      "Accuracy for U_14 pca8 :0.9754137115839243\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0677 - val_loss: 0.0347\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0312 - val_loss: 0.0284\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0274 - val_loss: 0.0260\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0243 - val_loss: 0.0222\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0217 - val_loss: 0.0207\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0205 - val_loss: 0.0197\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0197 - val_loss: 0.0189\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0190 - val_loss: 0.0184\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0182 - val_loss: 0.0176\n",
      "WARNING:tensorflow:Layer flatten_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_14 autoencoder8\n",
      "iteration:  0  cost:  0.3572586296063401\n",
      "iteration:  10  cost:  0.19502081378433278\n",
      "iteration:  20  cost:  0.20977013776562026\n",
      "iteration:  30  cost:  0.21195527335313763\n",
      "iteration:  40  cost:  0.11075903707560085\n",
      "iteration:  50  cost:  0.13008799116386732\n",
      "iteration:  60  cost:  0.11451330637085343\n",
      "iteration:  70  cost:  0.10083753816851596\n",
      "iteration:  80  cost:  0.1093915683024726\n",
      "iteration:  90  cost:  0.10989801530048707\n",
      "iteration:  100  cost:  0.09837303827764397\n",
      "iteration:  110  cost:  0.1121862820059631\n",
      "iteration:  120  cost:  0.0801396764777206\n",
      "iteration:  130  cost:  0.08387410004447958\n",
      "iteration:  140  cost:  0.07184851728393073\n",
      "Accuracy for U_14 autoencoder8 :0.9706855791962175\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 resize256\n",
      "iteration:  0  cost:  0.5960445067441198\n",
      "iteration:  10  cost:  0.1463659383104196\n",
      "iteration:  20  cost:  0.13632372659031866\n",
      "iteration:  30  cost:  0.10195681024820946\n",
      "iteration:  40  cost:  0.025806482025001607\n",
      "iteration:  50  cost:  0.07086046522610201\n",
      "iteration:  60  cost:  0.03329860419756964\n",
      "iteration:  70  cost:  0.03661063521697161\n",
      "iteration:  80  cost:  0.06719061082971294\n",
      "iteration:  90  cost:  0.0662030678780513\n",
      "iteration:  100  cost:  0.02852886811375269\n",
      "iteration:  110  cost:  0.03869056001366259\n",
      "iteration:  120  cost:  0.03322155371917846\n",
      "iteration:  130  cost:  0.04266469469572694\n",
      "iteration:  140  cost:  0.042815154804603586\n",
      "Accuracy for U_15 resize256 :0.9867612293144208\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 pca8\n",
      "iteration:  0  cost:  0.345142036927165\n",
      "iteration:  10  cost:  0.30860552832461313\n",
      "iteration:  20  cost:  0.2056414882801136\n",
      "iteration:  30  cost:  0.18443275686927116\n",
      "iteration:  40  cost:  0.22520527169481463\n",
      "iteration:  50  cost:  0.16745254664474277\n",
      "iteration:  60  cost:  0.07994637814889907\n",
      "iteration:  70  cost:  0.06133240841399048\n",
      "iteration:  80  cost:  0.02790395197901531\n",
      "iteration:  90  cost:  0.052827421328343734\n",
      "iteration:  100  cost:  0.027615598550350676\n",
      "iteration:  110  cost:  0.04470942925369878\n",
      "iteration:  120  cost:  0.013090673449343226\n",
      "iteration:  130  cost:  0.03262411790088247\n",
      "iteration:  140  cost:  0.020719252848874894\n",
      "Accuracy for U_15 pca8 :0.9787234042553191\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 54us/sample - loss: 0.0652 - val_loss: 0.0360\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0303 - val_loss: 0.0274\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0262 - val_loss: 0.0248\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0238 - val_loss: 0.0225\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0220 - val_loss: 0.0211\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0209 - val_loss: 0.0201\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0200 - val_loss: 0.0194\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0190 - val_loss: 0.0184\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0186 - val_loss: 0.0182\n",
      "WARNING:tensorflow:Layer flatten_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_15 autoencoder8\n",
      "iteration:  0  cost:  0.1607200299821103\n",
      "iteration:  10  cost:  0.10707737984189969\n",
      "iteration:  20  cost:  0.04105760161061437\n",
      "iteration:  30  cost:  0.04436563552708688\n",
      "iteration:  40  cost:  0.029634263644470325\n",
      "iteration:  50  cost:  0.030843134844376664\n",
      "iteration:  60  cost:  0.030263129330024053\n",
      "iteration:  70  cost:  0.02616273426498309\n",
      "iteration:  80  cost:  0.014233070226215158\n",
      "iteration:  90  cost:  0.025472571026347094\n",
      "iteration:  100  cost:  0.05569407544679674\n",
      "iteration:  110  cost:  0.019367488019393107\n",
      "iteration:  120  cost:  0.02061968137896745\n",
      "iteration:  130  cost:  0.010410629662882436\n",
      "iteration:  140  cost:  0.03161753524011306\n",
      "Accuracy for U_15 autoencoder8 :0.992434988179669\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 resize256\n",
      "iteration:  0  cost:  0.3553435337738612\n",
      "iteration:  10  cost:  0.3320599358207073\n",
      "iteration:  20  cost:  0.2607467730973488\n",
      "iteration:  30  cost:  0.2345352265007858\n",
      "iteration:  40  cost:  0.1760638249874835\n",
      "iteration:  50  cost:  0.17529976419094853\n",
      "iteration:  60  cost:  0.1376289589976074\n",
      "iteration:  70  cost:  0.11783670379777396\n",
      "iteration:  80  cost:  0.05188027692247545\n",
      "iteration:  90  cost:  0.06261785013552527\n",
      "iteration:  100  cost:  0.08637153358941217\n",
      "iteration:  110  cost:  0.05399929140311745\n",
      "iteration:  120  cost:  0.12585591703101087\n",
      "iteration:  130  cost:  0.07089801226979094\n",
      "iteration:  140  cost:  0.07961803292353989\n",
      "Accuracy for U_SO4 resize256 :0.9588652482269504\n",
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 pca8\n",
      "iteration:  0  cost:  0.45507778856748565\n",
      "iteration:  10  cost:  0.2569792331391362\n",
      "iteration:  20  cost:  0.2122899725570405\n",
      "iteration:  30  cost:  0.13300722057913617\n",
      "iteration:  40  cost:  0.03204412125693314\n",
      "iteration:  50  cost:  0.07171122393357927\n",
      "iteration:  60  cost:  0.036781085781811364\n",
      "iteration:  70  cost:  0.04777072545794585\n",
      "iteration:  80  cost:  0.04293420139581972\n",
      "iteration:  90  cost:  0.01075423622674549\n",
      "iteration:  100  cost:  0.032612798879064944\n",
      "iteration:  110  cost:  0.02042109239567642\n",
      "iteration:  120  cost:  0.04761207219510358\n",
      "iteration:  130  cost:  0.018146379408688094\n",
      "iteration:  140  cost:  0.015643950048255556\n",
      "Accuracy for U_SO4 pca8 :0.9815602836879432\n",
      "Train on 12665 samples, validate on 2115 samples\n",
      "Epoch 1/10\n",
      "12665/12665 [==============================] - 1s 55us/sample - loss: 0.0647 - val_loss: 0.0378\n",
      "Epoch 2/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0311 - val_loss: 0.0270\n",
      "Epoch 3/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0257 - val_loss: 0.0241\n",
      "Epoch 4/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0232 - val_loss: 0.0220\n",
      "Epoch 5/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0215 - val_loss: 0.0205\n",
      "Epoch 6/10\n",
      "12665/12665 [==============================] - 0s 37us/sample - loss: 0.0203 - val_loss: 0.0195\n",
      "Epoch 7/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0195 - val_loss: 0.0189\n",
      "Epoch 8/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0190 - val_loss: 0.0185\n",
      "Epoch 9/10\n",
      "12665/12665 [==============================] - 0s 38us/sample - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 10/10\n",
      "12665/12665 [==============================] - 1s 48us/sample - loss: 0.0183 - val_loss: 0.0179\n",
      "WARNING:tensorflow:Layer flatten_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss History for Hierarchical circuits, U_SO4 autoencoder8\n",
      "iteration:  0  cost:  0.3510546701533224\n",
      "iteration:  10  cost:  0.09761463420025823\n",
      "iteration:  20  cost:  0.02397880549625883\n",
      "iteration:  30  cost:  0.013904913720665116\n",
      "iteration:  40  cost:  0.019979492217430412\n",
      "iteration:  50  cost:  0.05444298196984325\n",
      "iteration:  60  cost:  0.02591592070788897\n",
      "iteration:  70  cost:  0.03392110830378884\n",
      "iteration:  80  cost:  0.018468827989503687\n",
      "iteration:  90  cost:  0.03045450012979467\n",
      "iteration:  100  cost:  0.01612231112560783\n",
      "iteration:  110  cost:  0.01679955287509967\n",
      "iteration:  120  cost:  0.034568265798485064\n",
      "iteration:  130  cost:  0.011824425365508871\n",
      "iteration:  140  cost:  0.02467169435996436\n",
      "Accuracy for U_SO4 autoencoder8 :0.9877068557919622\n"
     ]
    }
   ],
   "source": [
    "Benchmarking(Unitaries, U_num_params, Encodings, circuit, binary = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
